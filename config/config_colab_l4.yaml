# TinySigLIP Training Configuration for Google Colab L4
# L4 has 24GB VRAM, single GPU
# Inherits from config.yaml and only overrides specific settings

defaults:
  - config  # Inherit from config.yaml
  - _self_

# Training configuration overrides for L4 (24GB VRAM)
training:
  batch_size: 64  # Per GPU batch size (L4 has 24GB, can handle larger batches)
  max_steps: 5000  # Reasonable number of steps for Colab
  learning_rate: 5.0e-5  # Standard learning rate for single GPU
  warmup_steps: 200  # ~4% of max_steps

# Dataset configuration overrides
dataset:
  split: "val"  # Use 'val' for Colab (smaller dataset, faster)
  streaming: true  # Keep streaming to save memory
  num_workers: 4  # More workers for L4 (better CPU resources)

# Early stopping configuration
early_stopping:
  patience: 50  # Moderate patience for Colab
